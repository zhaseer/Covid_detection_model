# =============================
# Importing Libraries
# =============================
import os
import math
import random
import numpy as np
import pandas as pd
import tensorflow as tf
import keras
import cv2
import joblib
import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import recall_score, confusion_matrix
from PIL import ImageFilter

# =============================
# Reproducibility
# =============================
seed_value = 42
random.seed(seed_value)
np.random.seed(seed_value)
tf.random.set_seed(seed_value)

# =============================
# Image Configuration
# =============================
height, width = 224, 224
batch_size = 64

TRAINING_DIR = "X-ray Data/train"
TESTING_DIR = "X-ray Data/test"

# =============================
# Data Generator (Normal)
# =============================
def generate_data(DIR):
    datagen = ImageDataGenerator(rescale=1./255)

    generator = datagen.flow_from_directory(
        DIR,
        batch_size=batch_size,
        shuffle=True,
        seed=42,
        class_mode='sparse',
        target_size=(height, width),
        classes={'Normal': 0, 'Viral Pneumonia': 1, 'Covid': 2}
    )
    return generator

train_data = generate_data(TRAINING_DIR)
test_data = generate_data(TESTING_DIR)

# =============================
# Dataset Distribution Plot
# =============================
total_labels = np.concatenate([train_data.labels, test_data.labels])

counts = {
    'Normal': np.sum(total_labels == 0),
    'Viral Pneumonia': np.sum(total_labels == 1),
    'Covid': np.sum(total_labels == 2)
}

plt.bar(counts.keys(), counts.values())
plt.xlabel("Classes")
plt.ylabel("Counts")
plt.title("Distribution of X-ray Images")
plt.show()

# =============================
# Augmented Data Generator
# =============================
def generate_data_augmented(DIR):
    datagen = ImageDataGenerator(
        rescale=1./255,
        zoom_range=0.2,
        rotation_range=25,
        width_shift_range=0.15,
        height_shift_range=0.15,
        horizontal_flip=True
    )

    generator = datagen.flow_from_directory(
        DIR,
        batch_size=batch_size,
        seed=42,
        class_mode='sparse',
        target_size=(height, width),
        classes={'Normal': 0, 'Viral Pneumonia': 1, 'Covid': 2}
    )
    return generator

aug_train_data = generate_data_augmented(TRAINING_DIR)

# =============================
# Model Architecture (VGG16)
# =============================
tf.keras.backend.clear_session()

base_model = tf.keras.applications.VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(height, width, 3)
)
base_model.trainable = False

model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss='SparseCategoricalCrossentropy',
    metrics=['accuracy']
)

model.summary()

# =============================
# Callbacks
# =============================
os.makedirs("model", exist_ok=True)

checkpoint = tf.keras.callbacks.ModelCheckpoint(
    "model/best_model.keras",
    monitor='accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
)

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=10,
    restore_best_weights=True
)

# =============================
# Training
# =============================
history = model.fit(
    aug_train_data,
    validation_data=test_data,
    epochs=50,
    callbacks=[checkpoint, early_stop],
    verbose=1
)

# =============================
# Evaluation (Train Data)
# =============================
x_train, y_train = [], []

for i in range(math.ceil(len(aug_train_data.classes) / batch_size)):
    x_train.append(aug_train_data[i][0])
    y_train = np.concatenate((y_train, aug_train_data[i][1]))

x_train = np.concatenate(x_train, axis=0)

y_pred_train = np.argmax(model.predict(x_train), axis=1)
train_score = recall_score(y_train, y_pred_train, average='macro')

print("Train Recall Score:", round(train_score * 100, 2))

sns.heatmap(
    confusion_matrix(y_train, y_pred_train),
    annot=True,
    cmap="Blues",
    fmt="g",
    xticklabels=['Normal', 'Viral Pneumonia', 'Covid'],
    yticklabels=['Normal', 'Viral Pneumonia', 'Covid']
)
plt.title("Train Confusion Matrix")
plt.show()

# =============================
# Evaluation (Test Data)
# =============================
x_test, y_test = [], []

for i in range(math.ceil(len(test_data.classes) / batch_size)):
    x_test.append(test_data[i][0])
    y_test = np.concatenate((y_test, test_data[i][1]))

x_test = np.concatenate(x_test, axis=0)

y_pred_test = np.argmax(model.predict(x_test), axis=1)
test_score = recall_score(y_test, y_pred_test, average='macro')

print("Test Recall Score:", round(test_score * 100, 2))

sns.heatmap(
    confusion_matrix(y_test, y_pred_test),
    annot=True,
    cmap="Blues",
    fmt="g",
    xticklabels=['Normal', 'Viral Pneumonia', 'Covid'],
    yticklabels=['Normal', 'Viral Pneumonia', 'Covid']
)
plt.title("Test Confusion Matrix")
plt.show()
